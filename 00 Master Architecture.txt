Got you – this doc still screams “Postgres/pgvector” and we need it to say “Azure-native, enterprise, staged MVP” without making it look like we backtracked. I’ll give you **ready-to-paste replacements** for the key sections:

I’ll focus on:

* Exec summary / tenets
* Design updates table
* Layout Registry + Data Models & APIs
* Perf & scalability + workplan
* ADR 10.1 + Glossary

You can copy them straight into Word.

---

## 1️⃣ Executive Summary – updated wording

**Replace your existing Executive Summary block with:**

> We’re strengthening the existing Azure Textract-centric AE pipeline by adding a thin, modular intelligence layer on top: a Layout Registry, hybrid embeddings, and a tiered classification layer that can safely handle “infinite layouts” while controlling cost and complexity.
>
> For the MVP, the registry is implemented using **Azure-native services** (Azure Table Storage + Blob Storage + in-app vector similarity) rather than a dedicated database. This keeps the design lightweight, fast to provision, and aligned with enterprise guidance to favor managed cloud services over one-off Postgres instances.
>
> As volume and the number of layouts grow, the same design scales to **Azure AI Search or Azure Data Explorer** as the vector index, without changing the registry or classification logic. Clustering (ADAH) and client-specific fine-tuning remain part of the roadmap for high-volume and premium accuracy scenarios, layered on top of the MVP.
>
> These changes directly answer leadership guidance: optimize accuracy, auditability, and repeatability, while staying within the Azure ecosystem and minimizing operational overhead.

---

## 2️⃣ Key Design Tenets – add Azure-native + staged capacity

Under **2.1 Key Design Tenets**, add/replace bullets like this:

```text
• Enhance, don't replace: Build on the existing Textract investment and TFS pipelines.
• Accuracy first: Multi-source arbitration with measurable improvements (≥5pp).
• Learn once, reuse forever: Layout Registry prevents re-solving the same forms.
• Azure-native first: Prefer Azure Table Storage, Blob, Azure AI Search, and Data Explorer over net-new database stacks unless scale clearly requires them.
• Explainable decisions: Every extraction choice is auditable with scores and policies.
• Progressive delivery: MVP uses a lightweight registry (Table + Blob + in-app KNN); Phase-2 can introduce Azure AI Search / ADAH clustering as needed.
```

---

## 3️⃣ Design Updates Table (3.2.1) – remove pgvector wording

**Replace the “Scalability” row with:**

| Concern                 | Problem                        | Design Update / Solution                                                                                 | Location on Diagram                 | Status                                        |
| ----------------------- | ------------------------------ | -------------------------------------------------------------------------------------------------------- | ----------------------------------- | --------------------------------------------- |
| Classification Accuracy | Single classifier forces match | Added hierarchical classification + three-tier confidence system                                         | Step 3 (KNN + Classification logic) | Implemented                                   |
| Embedding Quality       | Text embeddings ignore layout  | Hybrid embedding (Text + Geometry), future LayoutLMv3                                                    | Step 2                              | Implemented                                   |
| Cost Efficiency         | Double OpenAI calls            | Fingerprint tier skips OpenAI for repeats                                                                | Step 1                              | Implemented                                   |
| Scalability             | KNN noisy at >50 categories    | Pre-filtering by doc_type + in-app KNN over a small candidate set; Azure AI Search/ADX as scale-out path | Step 3                              | MVP: Implemented (in-app); Scale-out: Roadmap |

Note: “in-app KNN” = vector similarity running in the service code, not in DB.

---

## 4️⃣ Layout Registry Component (3.3.1) – Azure-native, Postgres optional

**Replace 3.3.1 with:**

### 3.3.1 Layout Registry

* **Purpose:** Identify document layouts by similarity so we can reuse known extraction behavior instead of treating every AE form as new.
* **MVP Technology:**

  * **Azure Table Storage** – registry metadata for layouts, policies, and audit logs
  * **Azure Blob Storage** – persisted embedding vectors and artifacts
  * **In-app vector similarity** – KNN search implemented in the application layer over a small candidate set
* **Scale-out Options (Phase-2):**

  * **Azure AI Search (vector index)** or
  * **Azure Data Explorer (KNN over vectors)**
    used when the number of layouts and queries warrants a managed vector service.
* **Key Capability:** Match an incoming document to a known layout (or decide it’s “unknown/default”) fast enough to keep overall P95 latency under 60 seconds for a 10-page document.
* **MVP Status:** Critical path.

**Why It Matters**

* Encodes “learn once, reuse many times”: once a layout is seen and validated, the registry lets us recognize it again without re-solving the problem.
* Keeps the **MVP** simple and Azure-native while leaving a clear path to enterprise-grade vector indexing when Thermo Fisher’s volume justifies it.

---

## 5️⃣ Data Models & APIs (3.4.1) – remove Postgres focus

**Replace 3.4.1 with:**

### 3.4.1 Data Models & APIs

This concern guarantees consistency and interoperability across all components of the pipeline, regardless of the underlying storage engine.

* The registry data model is defined in a **storage-agnostic way**, with concrete mappings for:

  * **Azure Table Storage** (MVP) – entities for layouts, layout_policies, registry_audit_log, and layout-level metadata.
  * **Azure Blob Storage** – embedding vectors and large artifacts.
  * **Future**: Azure AI Search / Azure Data Explorer – vector index definitions for high-volume similarity search.
* API contracts are defined upfront for:

  * ingestion (`/registry/insert`, `/registry/fingerprint`)
  * similarity search (`/registry/knn`)
  * arbitration and publishing integration
    using JSON schema validation and standardized error codes.
* Message formats are defined for event-driven communication (Service Bus) and HITL feedback loops, including how case IDs, trace IDs, and similarity tiers are propagated.

This ensures we can swap or extend the physical storage (e.g., add Azure AI Search) without changing the logical contract between services.

---

## 6️⃣ Performance & Scalability (6.2) – remove pgvector callout

In **“Design Considerations”** under 6.2, replace the vector bullet:

```text
From Vector Index Optimization Perspective – pgvector indexing and clustering ensure fast layout retrieval even at scale.
```

with:

```text
From Vector Index Optimization Perspective – the MVP uses in-app KNN over a narrow candidate set (pre-filtered by doc_type and client); Phase-2 can introduce Azure AI Search or Azure Data Explorer for managed vector indexing when layout volume and QPS justify it.
```

Optionally add a new bullet:

```text
From Storage Strategy Perspective – Azure-native services (Table Storage, Blob, AI Search/ADX) are preferred over introducing single-purpose databases, aligning with TFS platform standards and reducing operational overhead.
```

---

## 7️⃣ Workplan & Milestones (7) – align with Azure storage

**Month 1 – Week 1–2** → update the line:

> Postgres + pgvector setup; schema migrations; seed 100 sample layouts

to:

> Layout Registry MVP storage (Azure Table + Blob) provisioned; schema finalized; seed ~50–100 sample layouts for similarity and routing tests.

**Month 1 – Week 3** → update:

> Fingerprint generation; embedding pipeline (text-embedding-3-small)

to:

> Fingerprint generation; hybrid embedding pipeline (text + geometry); persistence to Blob/registry entities.

**Month 1 – Week 4** → update:

> KNN search endpoint; baseline performance tests (<100ms @ 50k vectors)

to:

> KNN search endpoint (in-app vector similarity over pre-filtered candidates); baseline performance tests for typical AE layout volumes (<100ms per KNN call at MVP scale). Phase-2 will add Azure AI Search/ADX if layout count and QPS require a managed vector index.

That gives you a clean story: same algorithm, storage different.

---

## 8️⃣ ADR 10.1 – replace pgvector decision

**Completely replace 10.1 with:**

### 10.1 Why Azure-native Registry (Table + Blob + Optional AI Search) vs. Dedicated Vector DB

**Decision:** Use Azure Table Storage + Blob Storage for the MVP registry, with an upgrade path to Azure AI Search or Azure Data Explorer for vector indexing, instead of standing up a dedicated Postgres/pgvector or third-party vector database.

**Context:**
We need KNN-style layout similarity for a relatively small number of layout templates in the MVP, with a clear path to scale as volume grows, while staying aligned with Thermo Fisher’s enterprise preferences for Azure-native services.

**Rationale:**

* **Enterprise alignment:** Uses Azure-native services TFS already supports (Table, Blob, AI Search/ADX) instead of introducing a new Postgres stack used only by one component.
* **MVP simplicity:** At initial scale (<50–100 layouts), in-app vector similarity over a small candidate set is simpler and cheaper than provisioning and operating a full vector database.
* **Cost & ops:** Avoids the overhead of provisioning, securing, and maintaining a dedicated Postgres instance used for a single registry table.
* **Future-proof:** All embeddings and fingerprints are stored in a backend-agnostic format, so we can introduce Azure AI Search or ADX for vector search later without changing the registry API.

**Trade-offs:**

* In-app KNN is suitable for MVP scale; high-QPS, high-cardinality workloads may require AI Search/ADX to hit aggressive SLAs.
* Some advanced pgvector operations (e.g., IVFFlat tuning) are not used; instead we rely on Azure Search or ADX semantics when we scale up.

**Status:** Approved for MVP; scale-out path will be revisited once we approach the agreed thresholds for layout count / traffic.

---

## 9️⃣ Glossary – remove pgvector, add Azure AI Search

In the **Glossary**, update these entries:

* **Embedding:**
  Instead of “768-dimensional vector representation of document layout”, you can say:

  > Dense numerical vector representation (e.g., 768-dimensional) of document layout/features used for similarity search.

* **Fingerprint:** keep as-is.

* **pgvector:**
  Either remove, or replace with:

  > **Vector Index:** A structure used by the system (in-app or via Azure AI Search / Azure Data Explorer) to perform fast KNN similarity search over embeddings.

Add:

> **Azure AI Search:** Azure-native search and vector indexing service used as the scale-out option for layout similarity when layout and traffic volumes grow.
> **Azure Table Storage:** Azure-native key-value / table service used to store registry metadata (layouts, policies, audit logs) in the MVP.

---

If you want, next step I can:

* Rewrite the **“3.3.2 Clustering (ADAH)”** section to clearly mark it as **Phase-2 / optional**, not critical path, so MVP doesn’t look over-engineered.
* Draft a **one-paragraph summary** you can use in meetings: “We used to talk about Postgres/pgvector; here’s how we’ve refactored to Azure-native without losing functionality.”
